
---



# -------------------------------------------------------------------
    Data:         # Use Cora dataset training model
        datatype:           'cora'  # Type of the citation data 
        save_results:       "graphsage_cora/detect" # Folder name to save the results 
        datapath :          "E:\\Freelance_projects\\GNN\\Tutsv2\\pyGNN_NC\\src\\dataset\\citation_base\\cora\\"   #Input data path for loading data
        model_save_path:    "E:\\Freelance_projects\\GNN\\Tutsv2\\pyGNN_NC_XAI_V2\\weights"  # Path to save the weight file 
        save_fig:           True  # Saving of the training and validation plot  
        output_viz:         True  # saving the results output     
# -------------------------------------------------------------------
    random_state :      42           # random seed
    graphsage :                         # dropedge parameter
        type :             'graphsage'               # type of base network    
    Model :  
        type:           'graphsage'                       # model parameters
        use_bn :        false          # whether to use batch normalization
        dropout :       0.5            # dropout ratio
        input_dim :     1433           # node feature dimensions
        hidden_dim :    128             # hidden layer output characteristic dimension
        output_dim :    7              # number of nodes category
        num_nodes :  2708
        num_class: 7
        enc1_num_samples : 5
        enc2_num_samples : 5
        
    Hyper :                         # training hyperparameter
        LR :            0.001          # optimizer initial learning rate
        epochs :        4          # training rounds
        Patience :      800            # stopped early rounds
        weight_decay :  0.0005         # optimizer weight decay
        batch_size:     64             # batch size for dataloading 
        order:  2           #Propagation step
        sample: 2               #Sampling times of dropnode
        tem:    0.3                 #Sharpening temperature
        lam:   0.7                 #Lamda

...